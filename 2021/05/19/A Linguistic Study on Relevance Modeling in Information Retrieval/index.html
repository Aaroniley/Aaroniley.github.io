<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>🎓A Linguistic Study on Relevance Modeling in Information Retrieval | yelin</title><meta name="author" content="叶临"><meta name="copyright" content="叶临"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ABSTRACTIR已经成为许多现实世界应用中的核心任务，例如网络搜索引擎，问题应答系统，会话机器人等IR任务中的相关性的定义和建模在信息科学和计算机科学研究领域始终是一直是关键挑战。具体来说，我们试图研究以下两个问题1、三种相关性统一模型(对应三种信息检索任务)在 对于文本的自然语言理解层面真的有不同吗？2、如果确实不同，如何将三种IR任务的内在异质性应用到 相关性模型中，并提高相关性模型的表">
<meta property="og:type" content="article">
<meta property="og:title" content="🎓A Linguistic Study on Relevance Modeling in Information Retrieval">
<meta property="og:url" content="https://aaroniley.github.io/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/index.html">
<meta property="og:site_name" content="yelin">
<meta property="og:description" content="ABSTRACTIR已经成为许多现实世界应用中的核心任务，例如网络搜索引擎，问题应答系统，会话机器人等IR任务中的相关性的定义和建模在信息科学和计算机科学研究领域始终是一直是关键挑战。具体来说，我们试图研究以下两个问题1、三种相关性统一模型(对应三种信息检索任务)在 对于文本的自然语言理解层面真的有不同吗？2、如果确实不同，如何将三种IR任务的内在异质性应用到 相关性模型中，并提高相关性模型的表">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2021-05-19T04:38:41.962Z">
<meta property="article:modified_time" content="2021-07-09T10:37:23.939Z">
<meta property="article:author" content="叶临">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/images/avator.png"><link rel="canonical" href="https://aaroniley.github.io/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '🎓A Linguistic Study on Relevance Modeling in Information Retrieval',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-07-09 18:37:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/images/avator.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">yelin</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">🎓A Linguistic Study on Relevance Modeling in Information Retrieval</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-05-19T04:38:41.962Z" title="发表于 2021-05-19 12:38:41">2021-05-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-07-09T10:37:23.939Z" title="更新于 2021-07-09 18:37:23">2021-07-09</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="🎓A Linguistic Study on Relevance Modeling in Information Retrieval"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><!-- <img src='/2021/05/19/a/索隆.png' width="50%"> -->
<h1 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h1><p>IR已经成为许多现实世界应用中的核心任务，例如网络搜索引擎，问题应答系统，会话机器人等<br>IR任务中的相关性的定义和建模在信息科学和计算机科学研究领域始终是一直是关键挑战。<br>具体来说，我们试图研究以下两个问题<br>1、三种相关性统一模型(对应三种信息检索任务)在 对于文本的自然语言理解层面真的有不同吗？<br>2、如果确实不同，如何将三种IR任务的内在异质性应用到 相关性模型中，并提高相关性模型的表现?</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h1 id="Retrieval-tasks-in-information-retrieval"><a href="#Retrieval-tasks-in-information-retrieval" class="headerlink" title="Retrieval tasks in information retrieval"></a>Retrieval tasks in information retrieval</h1><h2 id="Document-Retrieval"><a href="#Document-Retrieval" class="headerlink" title="Document Retrieval"></a>Document Retrieval</h2><p>查询和文档之间的长度异质性。<br>用户查询通常很短，而且不明确的意图，在大多数情况下仅包括几个关键词。平均查询长度约为2.35个词<br>这些文件通常从万维网收集，并且具有更长的文本长度，从多个句子到几个段落。</p>
<h2 id="Answer-Retrieval"><a href="#Answer-Retrieval" class="headerlink" title="Answer Retrieval"></a>Answer Retrieval</h2><p>问题通常是自然语言，这些语言是良好的句子，并具有更明确的意图描述<br>答案通常是短文本<br>答案不仅应该与局部相关但也正确解决问题<br>早期的统计方法专注于复杂的特征工程<br>近年来，端到端的神经模型已应用于相关性建模</p>
<h2 id="Response-Retrieval"><a href="#Response-Retrieval" class="headerlink" title="Response Retrieval"></a>Response Retrieval</h2><p>响应检索是自动对话系统中的核心任务，Apple Siri，Google Now,和Microsoft 小冰<br>对一个输入话语，从话存储库中选择正确的响应<br>在多轮响应检索中，每一个输入话语都存在一个对话背景，包含历史对话<br>输入话语和候选响应通常是短句子，这些句子在形式中均匀。<br>响应检索的相关性通常是指某些语义对应（或相干结构），这在定义上是很广泛的<br>例如，给定输入话语“omg我这么一大把岁数还得了近视眼”，响应可以从一般（例如，“真的吗？”）到特定的（例如，“是啊，希望收到一副眼镜作为礼物“）<br>因此，模拟一致性并避免在响应检索中的一般琐事反应（避免啰嗦）通常是至关重要的</p>
<h1 id="Probing-analysis（探测分析）"><a href="#Probing-analysis（探测分析）" class="headerlink" title="Probing analysis（探测分析）"></a>Probing analysis（探测分析）</h1><h2 id="The-Probing-Method"><a href="#The-Probing-Method" class="headerlink" title="The Probing Method"></a>The Probing Method</h2><p>分析每个IR任务的Bert-base和bert-ft之间的每个探测任务的性能差距</p>
<h2 id="Probing-Tasks"><a href="#Probing-Tasks" class="headerlink" title="Probing Tasks"></a>Probing Tasks</h2><h3 id="Lexical-Tasks（词法）"><a href="#Lexical-Tasks（词法）" class="headerlink" title="Lexical Tasks（词法）"></a>Lexical Tasks（词法）</h3><ul>
<li>词汇任务专注于词汇在句子，段落或文档中的词汇意义和位置</li>
<li>Text Chunking (Chunk) ：将复杂的文本划分为较小的部分。CoNLL 2000 dataset</li>
<li>Part-of-Speech Tagging：词性标注。UD-EWT dataset</li>
<li>Named Entity Recogntion (NER) ：命名实体识别。 CoNLL 2003 dataset</li>
</ul>
<h3 id="Syntactic-Tasks（语法）"><a href="#Syntactic-Tasks（语法）" class="headerlink" title="Syntactic Tasks（语法）"></a>Syntactic Tasks（语法）</h3><ul>
<li>处理句子中单词之间的关系，正确创建句子结构和单词顺序</li>
<li>Grammatical Error Detection (GED) ：语法错误检测。First Certificate in English dataset</li>
<li>Syntactic Dependence ：句法依赖。弧预测和弧分类UD-EWT dataset</li>
<li>句法弧依赖关系预测（Synarcpred）是二分类任务，识别两个词之间存在关系。</li>
<li>语法弧依赖关系分类（synarccls）是一个多分类任务，假设输入词彼此有关系，识别是哪类关系。</li>
</ul>
<h3 id="Semantic-Tasks（语义）"><a href="#Semantic-Tasks（语义）" class="headerlink" title="Semantic Tasks（语义）"></a>Semantic Tasks（语义）</h3><ul>
<li>Preposition Supersense Disambiguatio STREUSLE 4.0 corpus</li>
<li>n PS-fxn：介词功能分析。</li>
<li><p>PS-role：介词角色分析。</p>
</li>
<li><p>coreference arc prediction (CorefArcPred)  ：代词、实体共指预测.CoNLL dataset</p>
</li>
<li>Semantic Dependence：SemEval 2015 dataset</li>
<li>SemArcPred: semantic arc dependency prediction， 两个token之间是否语义依存</li>
<li><p>SemArcCls: semantic arc dependency classification， 两个token语义关系分类</p>
</li>
<li><p>Synonym &amp; Polysemy：同义词，一词多义。crawled 10k sentences</p>
</li>
<li>Keyword extraction：关键词提取。Inspec dataset</li>
<li>Topic Classification:主题分类。Yahoo! Answers dataset</li>
</ul>
<h2 id="Experimental-setting"><a href="#Experimental-setting" class="headerlink" title="Experimental setting"></a>Experimental setting</h2><h3 id="Retrieval-model"><a href="#Retrieval-model" class="headerlink" title="Retrieval model"></a>Retrieval model</h3><p>bert</p>
<h3 id="Retrieval-Datasets"><a href="#Retrieval-Datasets" class="headerlink" title="Retrieval Datasets."></a>Retrieval Datasets.</h3><p>Robust04 NDCG@20<br>MsMarco MRR@10<br>Ubuntu recall@1<br><img src="/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/t1.png" alt></p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="How-does-the-unified-retrieval-model-perform-on-each-retrieval-task-统一模型表现如何"><a href="#How-does-the-unified-retrieval-model-perform-on-each-retrieval-task-统一模型表现如何" class="headerlink" title="How does the unified retrieval model perform on each retrieval task?统一模型表现如何"></a>How does the unified retrieval model perform on each retrieval task?统一模型表现如何</h3><p><img src="/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/f1.png" alt></p>
<ol>
<li>预训练模型bert都取得了sota,预训练中学习到的语言学信息对相关度模型还是很有效果的，另外，bert并不一定最后一层效果最好，因此探测实验应该在所有层进行，而不仅仅是最后一层<br><img src="/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/t5.png" alt></li>
<li>Bert-ft &gt; Bert-base,分别提升23.3%, 45.3%和51.6%,表明Bert-ft能够在特定IR任务上学习到任务相关的特定属性。</li>
<li>Bert-ft 与Bert-base的差距随层数增加而增加，表明Bert-ft中更高的层倾向于学习任务特定特征，而下层学习基本语言特征</li>
</ol>
<h3 id="Do-different-IR-tasks-show-different-modeling-focuses-in-terms-of-natural-language-understanding-在自然语言理解方面，不同的任务是否体现了不同的模型关注点"><a href="#Do-different-IR-tasks-show-different-modeling-focuses-in-terms-of-natural-language-understanding-在自然语言理解方面，不同的任务是否体现了不同的模型关注点" class="headerlink" title="Do different IR tasks show different modeling focuses in terms of natural language understanding?在自然语言理解方面，不同的任务是否体现了不同的模型关注点"></a>Do different IR tasks show different modeling focuses in terms of natural language understanding?在自然语言理解方面，不同的任务是否体现了不同的模型关注点</h3><p>基于对探测任务的性能差距进行定量分析，我们对IR任务之间的差异进行研究。<br>从IR任务角度：<br><img src="/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/t2.png" alt></p>
<ol>
<li>document retrieval</li>
<li>answer retrieval</li>
<li>response retrieval<br>For response retrieval, it is surprising to see that the performances of most probing tasks (i.e., 12 out of 16) have been decreased by Bertrsp, among which ten drops are significant. It suggests that most linguistic properties encoded by the original Bert has already been sufficient for the relevance modeling in response retrieval. Meanwhile, we can find that Bertrspimproves Synonym while decreases Polysemy significantly, as two extremes. The results demonstrate that response retrieval need to better understand similar words in different contexts than to distinguish the same words in different context.</li>
</ol>
<p>从探测任务角度：<br>1、CorefArcPred 、Keyword、NER、GED 结果表明，文档检索中的相关性建模会更多地关注相似的关键字，而答案检索中的相关性建模会更多地关注识别问题和答案的目标实体。<br>2、Word Scramble 它表明，答案检索和响应检索比文档检索更关心词序和句子结构。Polysem、 Topic 文档检索和答案检索比响应检索更多地关注主题的理解<br>3、Synonym、 PS-role 捕获所有相关建模任务中的同义词是非常重要的</p>
<p>基于所有上述观察，我们可以得出结论，三个代表检索任务中的相关性建模显示了在自然语言理解方面的焦点相当不同</p>
<h3 id="Do-relevance-modeling-treat-their-inputs-differently-in-terms-of-natural-language-understanding-相关性建模在自然语言理解方面会区别对待他们的左右两个输入吗？"><a href="#Do-relevance-modeling-treat-their-inputs-differently-in-terms-of-natural-language-understanding-相关性建模在自然语言理解方面会区别对待他们的左右两个输入吗？" class="headerlink" title="Do relevance modeling treat their inputs differently in terms of natural language understanding?相关性建模在自然语言理解方面会区别对待他们的左右两个输入吗？"></a>Do relevance modeling treat their inputs differently in terms of natural language understanding?相关性建模在自然语言理解方面会区别对待他们的左右两个输入吗？</h3><p><img src="/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/f2.png" alt><br>document retrieval<br>answer retrieval<br>response retrieval<br>因此，我们可以发现答案检索（即6逆趋势）是语言视图中固有的最异构的，其次是文档检索（即，3逆趋势）和响应检索（即2反向趋势） ）。</p>
<h1 id="Intervention-analysis（干预分析）"><a href="#Intervention-analysis（干预分析）" class="headerlink" title="Intervention analysis（干预分析）"></a>Intervention analysis（干预分析）</h1><h2 id="Intervention-Settings"><a href="#Intervention-Settings" class="headerlink" title="Intervention Settings"></a>Intervention Settings</h2><p>基于以下三个原因选择四个代表性探测任务作为干预因素，即Keyword, NER, Synonym, and SemArcCls<br>Synonym在三个检索任务都提高了<br>SemArcCls在三个检索任务都降低了</p>
<p>Feature Intervention:我们将每个因子(e.g.,PER, ORG, LOC in the NER)的标签映射到嵌入空间并将嵌入BERT输入Embeddings<br>Parameter Intervention:先在 NLU tasks任务上进行微调BERT模型，然后进一步在 三个IR任务上面分别 Finetune 模型。 BERT上面多加了一层MLP适应各个 NLU任务。<br>Objective Intervention:干预因素以及检索任务进行多任务学习，我们在BERT模型的顶部为每个干预因子添加特定于任务的层。如在BERT顶部添加CRF层，用于序列标记任务（即，ner），并在BERT顶部添加线性层，用于分类任务（即，Keyword, SemArcCls, and Synonym）<br><img src="/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/t3.png" alt></p>
<h2 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h2><p><img src="/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/t4.png" alt></p>
<h3 id="Intervention-Methods-Comparison"><a href="#Intervention-Methods-Comparison" class="headerlink" title="Intervention Methods Comparison."></a>Intervention Methods Comparison.</h3><h3 id="Intervention-Factors-Analysis"><a href="#Intervention-Factors-Analysis" class="headerlink" title="Intervention Factors Analysis."></a>Intervention Factors Analysis.</h3><h1 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h1><h2 id="The-Relevance-Modeling"><a href="#The-Relevance-Modeling" class="headerlink" title="The Relevance Modeling"></a>The Relevance Modeling</h2><h2 id="The-Probing-Analysis"><a href="#The-Probing-Analysis" class="headerlink" title="The Probing Analysis"></a>The Probing Analysis</h2><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><h1 id="精句摘要"><a href="#精句摘要" class="headerlink" title="精句摘要"></a>精句摘要</h1><p>A major characteristic of document retrieval is the length heterogeneity between queries and documents.<br>Compared with document retrieval, answer retrieval is more homogeneous and poses different challenges.</p>
<p><img src="/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/Data\个人网站\blog\source\_posts\t5.png" alt></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">叶临</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://aaroniley.github.io/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/">https://aaroniley.github.io/2021/05/19/A%20Linguistic%20Study%20on%20Relevance%20Modeling%20in%20Information%20Retrieval/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://aaroniley.github.io" target="_blank">yelin</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2021/07/09/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E7%AC%AC%E4%B8%83%E7%AB%A0-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">数据挖掘-第七章 关联规则</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/images/avator.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">叶临</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ABSTRACT"><span class="toc-number">1.</span> <span class="toc-text">ABSTRACT</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Retrieval-tasks-in-information-retrieval"><span class="toc-number">3.</span> <span class="toc-text">Retrieval tasks in information retrieval</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Document-Retrieval"><span class="toc-number">3.1.</span> <span class="toc-text">Document Retrieval</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Answer-Retrieval"><span class="toc-number">3.2.</span> <span class="toc-text">Answer Retrieval</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Response-Retrieval"><span class="toc-number">3.3.</span> <span class="toc-text">Response Retrieval</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Probing-analysis%EF%BC%88%E6%8E%A2%E6%B5%8B%E5%88%86%E6%9E%90%EF%BC%89"><span class="toc-number">4.</span> <span class="toc-text">Probing analysis（探测分析）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Probing-Method"><span class="toc-number">4.1.</span> <span class="toc-text">The Probing Method</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Probing-Tasks"><span class="toc-number">4.2.</span> <span class="toc-text">Probing Tasks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Lexical-Tasks%EF%BC%88%E8%AF%8D%E6%B3%95%EF%BC%89"><span class="toc-number">4.2.1.</span> <span class="toc-text">Lexical Tasks（词法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Syntactic-Tasks%EF%BC%88%E8%AF%AD%E6%B3%95%EF%BC%89"><span class="toc-number">4.2.2.</span> <span class="toc-text">Syntactic Tasks（语法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Semantic-Tasks%EF%BC%88%E8%AF%AD%E4%B9%89%EF%BC%89"><span class="toc-number">4.2.3.</span> <span class="toc-text">Semantic Tasks（语义）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experimental-setting"><span class="toc-number">4.3.</span> <span class="toc-text">Experimental setting</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Retrieval-model"><span class="toc-number">4.3.1.</span> <span class="toc-text">Retrieval model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Retrieval-Datasets"><span class="toc-number">4.3.2.</span> <span class="toc-text">Retrieval Datasets.</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results"><span class="toc-number">4.4.</span> <span class="toc-text">Results</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#How-does-the-unified-retrieval-model-perform-on-each-retrieval-task-%E7%BB%9F%E4%B8%80%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%8E%B0%E5%A6%82%E4%BD%95"><span class="toc-number">4.4.1.</span> <span class="toc-text">How does the unified retrieval model perform on each retrieval task?统一模型表现如何</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Do-different-IR-tasks-show-different-modeling-focuses-in-terms-of-natural-language-understanding-%E5%9C%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E6%96%B9%E9%9D%A2%EF%BC%8C%E4%B8%8D%E5%90%8C%E7%9A%84%E4%BB%BB%E5%8A%A1%E6%98%AF%E5%90%A6%E4%BD%93%E7%8E%B0%E4%BA%86%E4%B8%8D%E5%90%8C%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%85%B3%E6%B3%A8%E7%82%B9"><span class="toc-number">4.4.2.</span> <span class="toc-text">Do different IR tasks show different modeling focuses in terms of natural language understanding?在自然语言理解方面，不同的任务是否体现了不同的模型关注点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Do-relevance-modeling-treat-their-inputs-differently-in-terms-of-natural-language-understanding-%E7%9B%B8%E5%85%B3%E6%80%A7%E5%BB%BA%E6%A8%A1%E5%9C%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E6%96%B9%E9%9D%A2%E4%BC%9A%E5%8C%BA%E5%88%AB%E5%AF%B9%E5%BE%85%E4%BB%96%E4%BB%AC%E7%9A%84%E5%B7%A6%E5%8F%B3%E4%B8%A4%E4%B8%AA%E8%BE%93%E5%85%A5%E5%90%97%EF%BC%9F"><span class="toc-number">4.4.3.</span> <span class="toc-text">Do relevance modeling treat their inputs differently in terms of natural language understanding?相关性建模在自然语言理解方面会区别对待他们的左右两个输入吗？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Intervention-analysis%EF%BC%88%E5%B9%B2%E9%A2%84%E5%88%86%E6%9E%90%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">Intervention analysis（干预分析）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Intervention-Settings"><span class="toc-number">5.1.</span> <span class="toc-text">Intervention Settings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results-1"><span class="toc-number">5.2.</span> <span class="toc-text">Results</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Intervention-Methods-Comparison"><span class="toc-number">5.2.1.</span> <span class="toc-text">Intervention Methods Comparison.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Intervention-Factors-Analysis"><span class="toc-number">5.2.2.</span> <span class="toc-text">Intervention Factors Analysis.</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Related-works"><span class="toc-number">6.</span> <span class="toc-text">Related works</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Relevance-Modeling"><span class="toc-number">6.1.</span> <span class="toc-text">The Relevance Modeling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Probing-Analysis"><span class="toc-number">6.2.</span> <span class="toc-text">The Probing Analysis</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Conclusion"><span class="toc-number">7.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%B2%BE%E5%8F%A5%E6%91%98%E8%A6%81"><span class="toc-number">8.</span> <span class="toc-text">精句摘要</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/07/26/day1%20%E9%93%BE%E8%A1%A8/" title="day1 链表"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="day1 链表"/></a><div class="content"><a class="title" href="/2021/07/26/day1%20%E9%93%BE%E8%A1%A8/" title="day1 链表">day1 链表</a><time datetime="2021-07-26T00:51:53.000Z" title="发表于 2021-07-26 08:51:53">2021-07-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/07/24/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" title="预训练语言模型"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="预训练语言模型"/></a><div class="content"><a class="title" href="/2021/07/24/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" title="预训练语言模型">预训练语言模型</a><time datetime="2021-07-24T08:40:52.000Z" title="发表于 2021-07-24 16:40:52">2021-07-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9%E5%9B%9E%E5%BF%86/" title="神经网络基础知识点回忆"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="神经网络基础知识点回忆"/></a><div class="content"><a class="title" href="/2021/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9%E5%9B%9E%E5%BF%86/" title="神经网络基础知识点回忆">神经网络基础知识点回忆</a><time datetime="2021-07-16T02:16:48.000Z" title="发表于 2021-07-16 10:16:48">2021-07-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/07/13/%E5%AF%B9%E4%BA%8E%E5%A4%9A%E7%89%88%E6%9C%ACcuda%E5%88%87%E6%8D%A2%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%A4%E8%AF%86/" title="对于多版本cuda切换的一些认识"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="对于多版本cuda切换的一些认识"/></a><div class="content"><a class="title" href="/2021/07/13/%E5%AF%B9%E4%BA%8E%E5%A4%9A%E7%89%88%E6%9C%ACcuda%E5%88%87%E6%8D%A2%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%A4%E8%AF%86/" title="对于多版本cuda切换的一些认识">对于多版本cuda切换的一些认识</a><time datetime="2021-07-13T13:47:04.000Z" title="发表于 2021-07-13 21:47:04">2021-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/07/12/%E6%95%99%E7%A8%8B%EF%BC%9A%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82%E5%A6%82%E4%BD%95%E7%94%A8LSA%E3%80%81PSLA%E3%80%81LDA%E5%92%8Clda2vec%E8%BF%9B%E8%A1%8C%E4%B8%BB%E9%A2%98%E5%BB%BA%E6%A8%A1/" title="教程：一文读懂如何用LSA、PSLA、LDA和lda2vec进行主题建模"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="教程：一文读懂如何用LSA、PSLA、LDA和lda2vec进行主题建模"/></a><div class="content"><a class="title" href="/2021/07/12/%E6%95%99%E7%A8%8B%EF%BC%9A%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82%E5%A6%82%E4%BD%95%E7%94%A8LSA%E3%80%81PSLA%E3%80%81LDA%E5%92%8Clda2vec%E8%BF%9B%E8%A1%8C%E4%B8%BB%E9%A2%98%E5%BB%BA%E6%A8%A1/" title="教程：一文读懂如何用LSA、PSLA、LDA和lda2vec进行主题建模">教程：一文读懂如何用LSA、PSLA、LDA和lda2vec进行主题建模</a><time datetime="2021-07-11T16:29:08.000Z" title="发表于 2021-07-12 00:29:08">2021-07-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 叶临</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>